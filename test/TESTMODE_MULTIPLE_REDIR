


echo hello   <file1.txt | grep "error"cat < input.txt > temp.txt | grep "error" > errors.log >> all_logs.log | sort >> sorted_errors.log
echo "Hello World" > file1 > file2 | grep "Hello" > file3 >> file4 | tr 'a-z' 'A-Z' > uppercase.txt
cut -d, -f2 < data.csv > column2.txt | sort > sorted_column2.txt >> sorted_unique.txt | uniq > final_output.txt
tac < input.txt > reversed.txt | grep "pattern" > filtered.txt >> final_output.txt | wc -l > line_count.txt
cat << EOF > words1.txt > words2.txt | tr ' ' '\n' > split_words.txt | sort > sorted_words.txt >> word_freq.txt



cat hello                  < file1.txt
cat     file1.txt  | grep "error"
cat < file1.txt | grep "error" | awk '{print $2}' < file2.txt | sort > output1.txt | tee result.txt > final_output.txt
sort < data.txt | grep "important" < more_data.txt | awk '{print $1, $3}' | uniq > processed_output.txt | tee final_data.txt > final_report.txt
cat file1.txt | grep "data" < file2.txt | awk '{print $1}' < file3.txt | sort > sorted_data.txt | tee output.txt > final_output.txt
curl -s http://example.com | sed 's/<[^>]*>//g' < raw_data.txt | sort | tee result.txt > clean_output.txt > processed_data.txt
cat < file1.txt | grep "error" | awk '{print $3}' < file2.txt | sort > sorted_output.txt | tee final_output.txt > processed_output.txt
sort < data.csv | awk -F',' '{print $1, $2}' < more_data.txt | grep "important" | uniq > cleaned_data.txt | tee final_clean.txt > result.txt
cat < log.txt | grep "critical" | cut -d' ' -f2- < additional_logs.txt | awk '{print $1}' | tee filtered_logs.txt > final_log.txt
